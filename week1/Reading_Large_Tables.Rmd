---
title: "Reading Large Tables"
author: "Gareth R. White"
date: "13/11/2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Reading Large Tables
https://www.coursera.org/learn/r-programming/lecture/IHss1/reading-large-tables

# Reading in Larger Datasets with read.table
Read the help page for read.table()
Memorise this help page!
Make a rough calculation for the amount of memory needed. You need this much physical memory.
Set comment.char <- "" if there are no comment lines.

# Reading Large Tables
colClasses can make read.table() run much faster.
If all columns are the same, just set once,
colClasses <= "numeric"

To work out the classes:
```{r}
initial <- read.table("datatable.txt", nrows=100)
classes <- sapply(initial, class)
tabAll <- read.table("datatable.txt", colClasses = classes)
```

Set nrows to help compute memory useage
Use wc to count number of lines (rows).

## Know Thy System
- How much memory available?
- What other applications are running?
- Are other users logged in?
- What operating system?
- Is the OS 32 or 64 bit?

## Calculating Memory Requirements
Example:

1,500,00 rows
120 columns
All numeric.
1,500,000 x 120 x 8 bytes per numeric
1,440,000,000 bytes
2^20 bytes / MB
1,373.29
1.34 GB

Typically will need 2x this amount to read in.

